{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation with Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGjxgJDqqiqrVzgmvBhKhl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagobert42/NMT-Attention/blob/main/Neural_Machine_Translation_with_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3O2RwGmK9M9"
      },
      "source": [
        "## Neural Machine Translation with Attention\n",
        "\n",
        "The goal of this notebook is to implement the **RNNsearch-50** model, i.e., the encoder-decoder with attention system for any language pair different from English-German, German-English, English-French, and French-English. We choose **German-Italian** as an example language pair.\n",
        "\n",
        "Furthermore we loosely follow **test-driven development** (\"TDD\") paradigms to replicate the original system based on the paper:\n",
        "\n",
        "    Bahdanau, Cho & Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015.\n",
        "\n",
        "An in-depth walk-through of the project is given in the accompanying report which can be found here ADD LINK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhVOlKEwH9Ez"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "Please choose which dependencies to install to your environment. On re-runs you can ucheck the boxes to save some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz_I8H-r3VtB"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "torch = True #@param {type:\"boolean\"}\n",
        "if torch:\n",
        "    !pip install --upgrade torch\n",
        "\n",
        "torchtext = True #@param {type:\"boolean\"}\n",
        "if torchtext:\n",
        "    !pip install --upgrade torchtext\n",
        "\n",
        "spacy_nlp = True #@param {type:\"boolean\"}\n",
        "if spacy_nlp:\n",
        "    !pip install --upgrade spacy\n",
        "\n",
        "spacy_packages = True #@param {type:\"boolean\"}\n",
        "if spacy_packages:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    !python -m spacy download de_core_news_sm\n",
        "    !python -m spacy download it_core_news_sm\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBfa-ne0NAB7"
      },
      "source": [
        "Next let us import all the modules we are going to be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nCXITKIM9wc"
      },
      "source": [
        "# access to translation datasets\n",
        "import torchtext\n",
        "\n",
        "# model implementation\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# tokenization\n",
        "import spacy\n",
        "\n",
        "# utilities\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import uuid"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOK7mD3p1uan"
      },
      "source": [
        "# 2. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhSrQhBqtLra"
      },
      "source": [
        "### 2.1 Vocabulary\n",
        "\n",
        "The vocabulary converts words to indeces and vice versa. Unknown words are marked with the < U > token. The vocabulary can be filtered by word counts and provides methods for converting between sequences and sentences. It also takes an optional SpaCy NLP object which (when given) is used for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd2ZdAMQr_Er"
      },
      "source": [
        "class Vocab:\n",
        "    \"\"\"\n",
        "    A vocabulary holding dictionaries for converting words to indeces and back.\n",
        "    \"\"\"\n",
        "    class Entry:\n",
        "        def __init__(self, id):\n",
        "            \"\"\" An entry to the vocabulary. With index and count. \"\"\"\n",
        "            self.id = id\n",
        "            self.count = 1\n",
        "\n",
        "        def __repr__(self):\n",
        "            \"\"\" String prepresentation for printing. \"\"\"\n",
        "            return str((self.id, self.count))\n",
        "\n",
        "    def __init__(self, text=None, spacy_nlp=None):\n",
        "        \"\"\"\n",
        "        Creates a vocabulary over an input text in the form of\n",
        "        dictionaries for indeces and word counts. Hand in a\n",
        "        spacy_nlp object to make use of a SpaCy for tokenization.\n",
        "        \"\"\"\n",
        "        self.spacy_nlp = spacy_nlp\n",
        "        self.words = {'<S>':self.Entry(0),\n",
        "                      '<E>':self.Entry(1),\n",
        "                      '<U>':self.Entry(2),\n",
        "                      '<P>':self.Entry(3)}\n",
        "        self.ids = {0:'<S>',\n",
        "                    1:'<E>',\n",
        "                    2:'<U>',\n",
        "                    3:'<P>'}\n",
        "        self.next_id = 4\n",
        "\n",
        "        if text:\n",
        "            self.append(text)\n",
        "\n",
        "    def append(self, txt):\n",
        "        \"\"\" Adds a string token by token to the vocabulary. \"\"\"\n",
        "        # use SpaCy for tokenization if requested\n",
        "        if self.spacy_nlp:\n",
        "            for tok in self.spacy_nlp.tokenizer(txt):\n",
        "                word = tok.text\n",
        "                if word not in self.words.keys():\n",
        "                    self.words[word] = self.Entry(self.next_id)\n",
        "                    self.ids[self.next_id] = word\n",
        "                    self.next_id += 1\n",
        "                else:\n",
        "                    self.words[word].count += 1\n",
        "        else:\n",
        "            for word in txt.split():\n",
        "                if word not in self.words.keys():\n",
        "                    self.words[word] = self.Entry(self.next_id)\n",
        "                    self.ids[self.next_id] = word\n",
        "                    self.next_id += 1\n",
        "                else:\n",
        "                    self.words[word].count += 1\n",
        "\n",
        "    def filter(self, n_samples, descending=True):\n",
        "        \"\"\"\n",
        "        Reduces this vocabs dictionary to n_samples and\n",
        "        the 4 special tokens after sorting by word count.\n",
        "        Indeces remain untouched\n",
        "        \"\"\"\n",
        "        #exclude specials from filter\n",
        "        del self.words['<S>']\n",
        "        del self.words['<E>']\n",
        "        del self.words['<U>']\n",
        "        del self.words['<P>']\n",
        "        sorted_list = list(sorted(\n",
        "                self.words.items(),\n",
        "                key=lambda item: item[1].count,\n",
        "                reverse=descending))\n",
        "        self.words = {k: v for k, v in sorted_list[:n_samples]}\n",
        "        # add specials back in, in case they were filtered\n",
        "        self.words['<S>'] = self.Entry(0)\n",
        "        self.words['<E>'] = self.Entry(1)\n",
        "        self.words['<U>'] = self.Entry(2)\n",
        "        self.words['<P>'] = self.Entry(3)\n",
        "\n",
        "    def get_indeces(self, sentence):\n",
        "        \"\"\" Produces a representation from the indeces in this vocabulary. \"\"\"\n",
        "        END = 1\n",
        "        UNK = 2\n",
        "        if self.spacy_nlp:\n",
        "            seq = [self.words[tok.text].id if tok.text in self.words.keys() else UNK\n",
        "                for tok in self.spacy_nlp.tokenizer(sentence)]\n",
        "        else:\n",
        "            seq = [self.words[word].id if word in self.words.keys() else UNK\n",
        "                for word in sentence.split()]\n",
        "        seq.append(END)\n",
        "        return seq\n",
        "    \n",
        "    def get_sentence(self, indeces):\n",
        "        \"\"\"\n",
        "        Converts a list of indeces into a readable sentence\n",
        "        using words from this vocabulary.\n",
        "        \"\"\"\n",
        "        return ' '.join([self.ids[id] for id in indeces])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmlGaPcWxy1B"
      },
      "source": [
        "We make sure the vocabulary works as intended by running tests with a tiny corpus. Note that during development these tests were written **first** and subsequently we implemented the related functionality in the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peRNwanhyNey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063d0c0a-0b30-4a44-b647-50177c35731d"
      },
      "source": [
        "TEST_CORPUS_EN = \"\"\"The final project should implement a system \n",
        "        related to deep learning for NLP using the Py- Torch library \n",
        "        and test it. The project is documented in an ACL-style paper \n",
        "        that adheres to the standards of practice in computational \n",
        "        linguistics.\"\"\"\n",
        "\n",
        "test_vocab = Vocab(TEST_CORPUS_EN)\n",
        "\n",
        "# Basic Functionality\n",
        "assert(test_vocab.ids[0] == '<S>') # <S> should always be first\n",
        "assert(test_vocab.ids[4] == 'The')\n",
        "assert(test_vocab.words['The'].id == 4)\n",
        "assert(test_vocab.words['The'].count == 2)\n",
        "\n",
        "# Spacy Option\n",
        "english_nlp = spacy.load('en_core_web_sm')\n",
        "spacy_vocab = Vocab(TEST_CORPUS_EN, spacy_nlp=english_nlp)\n",
        "\n",
        "assert(spacy_vocab.ids[0] == '<S>')\n",
        "assert(spacy_vocab.ids[6] == 'project')\n",
        "assert(spacy_vocab.words['project'].id == 6)\n",
        "assert(spacy_vocab.words['project'].count == 2)\n",
        "\n",
        "# Vocabulary Filter\n",
        "top_30 = test_vocab\n",
        "top_30.filter(n_samples=30)\n",
        "# new size should be n_samples + 4 specials\n",
        "assert(len(top_30.words) == 34)\n",
        "\n",
        "# Sentence Vector Conversion\n",
        "vec = spacy_vocab.get_indeces(\"This is a test\")\n",
        "sent = spacy_vocab.get_sentence(vec)\n",
        "\n",
        "assert(vec == [2, 27, 9, 24, 1])\n",
        "assert(sent == '<U> is a test <E>')\n",
        "\n",
        "print('OK. No errors or asserts were triggered during testing :)')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK. No errors or asserts were triggered during testing :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6ZfauRUbvsh"
      },
      "source": [
        "### 2.2 Dataset\n",
        "\n",
        "For data we look to the Web Inventory of Transcribed and Translated Talks which comes as a torchtext dataset.  You can see some examples during the building process. On re-runs you can ucheck the box to save some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBvGNzPvmhtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed860bc-6a17-4652-aede-55b0f3ec1761"
      },
      "source": [
        "from torchtext.datasets import IWSLT2017\n",
        "LOAD_DATA = True #@param {type:\"boolean\"} \n",
        "#@markdown (ETA 2:15 min)\n",
        "if LOAD_DATA:\n",
        "    train_iter, test_iter, val_iter = IWSLT2017(\n",
        "        split=('train', 'test', 'valid'),\n",
        "        language_pair=('de', 'it'))\n",
        "    \n",
        "    train_pairs = [(src, trg) for src, trg in train_iter]\n",
        "    test_pairs = [(src, trg) for src, trg in test_iter]\n",
        "    val_pairs = [(src, trg) for src, trg in val_iter]\n",
        "\n",
        "print('sentence pairs in...')\n",
        "print('...train:', len(train_pairs), end='\\t')\n",
        "print('...test:', len(test_pairs), end='\\t')\n",
        "print('...validation:', len(val_pairs))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2017-01-trnmted.tgz: 329MB [00:19, 16.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sentence pairs in...\n",
            "...train: 205465\t...test: 1567\t...validation: 923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JubRANtaqNZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2d0696-d033-4e15-c91c-bbc732b2fd95"
      },
      "source": [
        "BUILD_VOCAB = True #@param {type:\"boolean\"}\n",
        "if BUILD_VOCAB:\n",
        "\n",
        "    random.shuffle(train_pairs)\n",
        "    TRAIN_EXAMPLES = 50000 #@param {type:\"integer\"}\n",
        "    if TRAIN_EXAMPLES < len(train_pairs):\n",
        "        train_pairs = train_pairs[:TRAIN_EXAMPLES]\n",
        "\n",
        "    de_nlp = spacy.load('de_core_news_sm')\n",
        "    it_nlp = spacy.load('it_core_news_sm')\n",
        "    de_vocab = Vocab(spacy_nlp=de_nlp)\n",
        "    it_vocab = Vocab(spacy_nlp=it_nlp)\n",
        "\n",
        "    PRINT_EVERY = 10000\n",
        "    for n, (src, trg) in enumerate(train_pairs):\n",
        "        if n % PRINT_EVERY == 0:\n",
        "            clear_output()\n",
        "            print(\"Building vocabularies...\\n\")\n",
        "            print(f'Example: {n}\\n')\n",
        "            print('source ->', src)\n",
        "            print('target ->', trg)\n",
        "        de_vocab.append(src)\n",
        "        it_vocab.append(trg)\n",
        "\n",
        "    VOCAB_SIZE = 10000 #@param {type:\"integer\"}\n",
        "    de_vocab.filter(VOCAB_SIZE)\n",
        "    it_vocab.filter(VOCAB_SIZE)\n",
        "\n",
        "    def get_tensors(sentence_pairs, src_vocab, trg_vocab):\n",
        "        data = []\n",
        "        for n, (src, trg) in enumerate(sentence_pairs):\n",
        "            de_tensor = torch.tensor(src_vocab.get_indeces(src),\n",
        "                                    dtype=torch.long)\n",
        "            it_tensor = torch.tensor(trg_vocab.get_indeces(trg),\n",
        "                                    dtype=torch.long)\n",
        "            if n % PRINT_EVERY == 0:\n",
        "                clear_output()\n",
        "                print(\"Converting to tensors...\\n\")\n",
        "                print(f'Example: {n}\\n')\n",
        "                print('source ->', de_tensor)\n",
        "                print('target ->', it_tensor)\n",
        "            data.append((de_tensor, it_tensor))\n",
        "        return data\n",
        "        \n",
        "    train_data = get_tensors(train_pairs, de_vocab, it_vocab)\n",
        "    test_data = get_tensors(test_pairs, de_vocab, it_vocab)\n",
        "    val_data = get_tensors(val_pairs, de_vocab, it_vocab)\n",
        "    clear_output()\n",
        "\n",
        "# print a random example\n",
        "#@markdown (ETA 1:45 min)\n",
        "x = random.randrange(0, len(train_pairs))\n",
        "print(f'Example: {x}\\n')\n",
        "print('source ->', train_pairs[x][0])\n",
        "print('target ->', train_pairs[x][1])\n",
        "print(\"As tensors...\\n\")\n",
        "print('source ->', train_data[x][0], '\\n')\n",
        "print('target ->', train_data[x][1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example: 40582\n",
            "\n",
            "source -> Und jetzt schlieÃŸt sich der Kreis.\n",
            "\n",
            "target -> Stiamo per chiudere il cerchio.\n",
            "\n",
            "As tensors...\n",
            "\n",
            "source -> tensor([   4,  331, 8826,   47,   66, 6464,   26,   27,    1]) \n",
            "\n",
            "target -> tensor([2018,   46,  425,  114, 6528,   27,   28,    1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB03OWaY1m-K"
      },
      "source": [
        "# 3. Model\n",
        "\n",
        "The network consists of three main parts. These are the Encoder, Decoder and Alignment models which are ultimately combined to form the RNNsearch model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdkcx40MDXWh"
      },
      "source": [
        "## 3.1 Encoder\n",
        "\n",
        "**Inputs:** sequence of one-hot vectors representing a sentence\n",
        "\n",
        "**Outputs:** series of annotations, most recent GRU hidden states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kOKZvkaEB2Z"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.biRNN = nn.GRU(embedding_dim, hidden_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        outputs, hidden = self.biRNN(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        return outputs, hidden"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KpXEWJODXRi"
      },
      "source": [
        "## 3.2 Attention\n",
        "\n",
        "**Inputs:** encoder annotations, recent encoder hidden states\n",
        "\n",
        "**Outputs:** attention energies over the input sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw0jqTl_D_rD"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim, attention_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(hidden_dim * 3, attention_dim)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attention(torch.cat((\n",
        "            repeated_decoder_hidden,\n",
        "            encoder_outputs),\n",
        "            dim = 2)))\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_vvTUdUCzN7"
      },
      "source": [
        "## 3.3 Decoder\n",
        "\n",
        "**Input:** encoder annotations, recent encoder hidden states\n",
        "\n",
        "**Outputs:** current prediction, hidden states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9AVLDyND8Yg"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, attention_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.attention = Attention(hidden_dim, attention_dim)\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        self.rnn = nn.GRU((hidden_dim * 2) + embedding_dim, hidden_dim)\n",
        "        self.out = nn.Linear((hidden_dim * 3) + embedding_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def apply_attention(self, decoder_hidden, encoder_outputs):\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "    def forward(self, input, decoder_hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        weighted_encoder_rep = self.apply_attention(decoder_hidden,\n",
        "                                                    encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "        output = self.out(torch.cat((output,\n",
        "                                     weighted_encoder_rep,\n",
        "                                     embedded), dim = 1))\n",
        "        return output, decoder_hidden.squeeze(0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IrOJs9lDXbX"
      },
      "source": [
        "## 3.4 Final Model\n",
        "\n",
        "**Input:** encoded source sequence, empty target sequence\n",
        "\n",
        "**Outputs:** encoded translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7e88XNmEIGV"
      },
      "source": [
        "class Translator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dim, attention_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        self.target_vocab_size = output_dim\n",
        "        self.encoder = Encoder(input_dim, embedding_dim, hidden_dim, dropout)\n",
        "        self.decoder = Decoder(output_dim, embedding_dim, hidden_dim, attention_dim, dropout)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, y, teacher_forcing_ratio=0.5):\n",
        "        batch_size = x.shape[1]\n",
        "        max_len = y.shape[0]\n",
        "        outputs = torch.zeros(max_len, batch_size, self.target_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(x)\n",
        "        \n",
        "        # first input to the decoder is the <S> token\n",
        "        output = y[0,:]\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (y[t] if teacher_force else top1)\n",
        "        return outputs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG2sSJlEESQi"
      },
      "source": [
        "# 4. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqJrAS3AZ1as"
      },
      "source": [
        "## 4.2 Batching\n",
        "\n",
        "In the original training method 1600 sentence pairs are retrieved at every 20th update and sorted by length. Then 20 new mini-batches with 80 sentence pairs each are prepared from this data. This keeps the average length of batches comparably low since each batch is always the length of its longest sequence while all of its smaller sequences are being padded with the special token < P >."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0eaxpk1PTs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5308291-d47a-46b9-984e-2afe96cd167d"
      },
      "source": [
        "RESORT_DATA = True #@param {type:\"boolean\"}\n",
        "if RESORT_DATA:\n",
        "\n",
        "    #filter input sentences for max length\n",
        "    MAX_SENT_LENGTH = 50 #@param {type:\"integer\"}\n",
        "    train_data_pruned = []\n",
        "    for pair in train_data:\n",
        "        if pair[0].size()[0] < MAX_SENT_LENGTH:\n",
        "            train_data_pruned.append(pair)\n",
        "    print(f'Sentence pairs of size < {MAX_SENT_LENGTH}:',\n",
        "          len(train_data_pruned))\n",
        "\n",
        "    train_sorted = list(sorted(\n",
        "                    train_data_pruned,\n",
        "                    key=lambda pair: pair[0].size()[0],\n",
        "                    reverse=True))\n",
        "\n",
        "    test_sorted = list(sorted(\n",
        "                    test_data,\n",
        "                    key=lambda pair: pair[0].size()[0],\n",
        "                    reverse=True))\n",
        "\n",
        "    val_sorted = list(sorted(\n",
        "                    val_data,\n",
        "                    key=lambda pair: pair[0].size()[0],\n",
        "                    reverse=True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence pairs of size < 50: 47970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d4mFjO9ci0I"
      },
      "source": [
        "def prepare_batches(data):\n",
        "    \"\"\" Returns mini-batches from sorted data. \"\"\"\n",
        "    STA = 0.0\n",
        "    PAD = 3.0\n",
        "    de_batch = []\n",
        "    it_batch = []\n",
        "    for de_item, it_item in data:\n",
        "        de_batch.append(torch.cat([torch.tensor([STA]), de_item], dim=0))\n",
        "        it_batch.append(torch.cat([torch.tensor([STA]), it_item], dim=0))\n",
        "    de_batch_padded = pad_sequence(de_batch, padding_value=PAD)\n",
        "    it_batch_padded = pad_sequence(it_batch, padding_value=PAD)\n",
        "    return de_batch_padded, it_batch_padded\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16 #@param {type:\"integer\"}\n",
        "train_batches = DataLoader(train_sorted, batch_size=BATCH_SIZE,\n",
        "                           collate_fn=prepare_batches)\n",
        "test_batches = DataLoader(test_sorted, batch_size=BATCH_SIZE,\n",
        "                          collate_fn=prepare_batches)\n",
        "val_batches = DataLoader(val_sorted, batch_size=BATCH_SIZE,\n",
        "                         collate_fn=prepare_batches)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOtwYOyD3D1J"
      },
      "source": [
        "## 4.2 Hyperparameters\n",
        "\n",
        "For our purposes we can adjust the hyperparameters to more reasonably expensive values in terms of training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nBOZXc-60N8",
        "cellView": "form"
      },
      "source": [
        "#@markdown Model Parameters\n",
        "EMBEDDING_DIM =  64#@param {type:\"integer\"}\n",
        "HIDDEN_DIM =  128#@param {type:\"integer\"}\n",
        "ATTENION_DIM =  64#@param {type:\"integer\"}\n",
        "DROPOUT = 0.5 #@param {type:\"number\"}\n",
        "#@markdown Training Parameters\n",
        "EPOCHS = 5 #@param {type:\"integer\"}\n",
        "RHO = 0.95 #@param {type:\"number\"}\n",
        "EPSILON = 0.000001 #@param {type:\"number\"}\n",
        "LEARNING_RATE = 0.01 #@param {type:\"number\"}\n",
        "GRADIENT_CLIP = 1 #@param {type:\"number\"}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twQkSVO1Upkt"
      },
      "source": [
        "## 4.3 Model Declaration\n",
        "\n",
        "Lets check the model that we can declare with these hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPLl4sl0vTRM"
      },
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        # recurrent layer weights\n",
        "        if 'weight_ih_l' in name or 'weight_hh_l' in name:\n",
        "            nn.init.orthogonal_(param.data)\n",
        "        # recurrent layer biases\n",
        "        elif 'bias_ih_l' in name or 'bias_hh_l' in name:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "        elif 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        elif 'bias' in name:\n",
        "            nn.init.constant_(param.data, 0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtIeU-IgUoY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0fb09ed-0c9f-48c7-a0a3-39fc2f8ab4b3"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "input_dim = len(de_vocab.words)\n",
        "output_dim = len(it_vocab.words)\n",
        "\n",
        "model = Translator(input_dim, output_dim, EMBEDDING_DIM, HIDDEN_DIM, ATTENION_DIM, DROPOUT, device).to(device)\n",
        "model.apply(init_weights)\n",
        "print(\"Initialized weights...\")\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {n_parameters} trainable parameters:')\n",
        "print(model)\n",
        "\n",
        "use_adadelta = False #@param {type:\"boolean\"}\n",
        "if use_adadelta:\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=RHO, eps=EPSILON)\n",
        "else:\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_func = nn.CrossEntropyLoss(ignore_index=de_vocab.words['<P>'].id)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized weights...\n",
            "The model has 6151636 trainable parameters:\n",
            "Translator(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(10004, 64)\n",
            "    (biRNN): GRU(64, 128, bidirectional=True)\n",
            "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (attention): Attention(\n",
            "      (attention): Linear(in_features=384, out_features=64, bias=True)\n",
            "    )\n",
            "    (embedding): Embedding(10004, 64)\n",
            "    (rnn): GRU(320, 128)\n",
            "    (out): Linear(in_features=448, out_features=10004, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Z-7zgXpm7o"
      },
      "source": [
        "## 4.4 Train Loop\n",
        "\n",
        "We initialize the weights as proposed. Setting all biases to 0, the recurrent weight matrices to random orthogonal matrices and sampling all other weights from normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91MPWODks-EK"
      },
      "source": [
        "def train(model, train_batches, optimizer, loss_func, gradient_clip):\n",
        "    epoch_loss = 0\n",
        "    n_batches = len(train_batches)\n",
        "    model.train()\n",
        "\n",
        "    for batch, (x, y) in enumerate(train_batches):\n",
        "        x = x.to(device=device, dtype=torch.long)\n",
        "        y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(x, y)\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        y = y[1:].view(-1)\n",
        "\n",
        "        loss = loss_func(output, y)\n",
        "\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), gradient_clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        # indicate progress\n",
        "        BAR_SIZE = 20\n",
        "        i = math.ceil(batch/n_batches * BAR_SIZE)\n",
        "        print('\\r', '#' * i, ' ' * (BAR_SIZE-i), '{:.2f}%'.format(batch/n_batches * 100.0), end=' ')\n",
        "\n",
        "    return epoch_loss / n_batches"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LY0WeyFi_1V"
      },
      "source": [
        "def evaluate(model, val_batches, loss_func):\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    n_batches = len(train_batches)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (x, y) in enumerate(val_batches):\n",
        "            x, trg = x.to(device), y.to(device)\n",
        "\n",
        "            output = model(x, y, 0) # no teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            y = y[1:].view(-1)\n",
        "\n",
        "            loss = loss_func(output, y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # indicate progress\n",
        "            BAR_SIZE = 20\n",
        "            i = math.ceil(batch/n_batches * BAR_SIZE)\n",
        "            print('\\r', '#' * i, ' ' * (BAR_SIZE-i), '{:.2f}%'.format(batch/n_batches * 100.0), end=' ')\n",
        "\n",
        "    return epoch_loss / len(val_batches)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLLSRe4oW9Sd"
      },
      "source": [
        "def time_since(start):\n",
        "    s = time.time() - start\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFPmzOZqpmL9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "1c00bafe-ea47-45bd-fb2c-8c4d83bcb0c5"
      },
      "source": [
        "print(\"Begin training...\")\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'[Epoch: {epoch+1}] @', datetime.now().strftime(\"%H:%M:%S\"))\n",
        "    train_loss = train(model, train_batches, optimizer, loss_func, GRADIENT_CLIP)\n",
        "    print('\\n[time elapsed: {} loss: {:.4f}]'.format(time_since(start_time), epoch, train_loss))\n",
        "print(\"Training finished...\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin training...\n",
            "[Epoch: 1] @ 15:42:50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-55af82dc27c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[Epoch: {epoch+1}] @'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRADIENT_CLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n[time elapsed: {} loss: {:.4f}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_since\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2c2698b5a79b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_batches, optimizer, loss_func, gradient_clip)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-996c004df2f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# first input to the decoder is the <S> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0a7ae8ba8f1c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 838\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    839\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxqDDE3A45Em"
      },
      "source": [
        "id = uuid.uuid1() + '.pt'\n",
        "torch.save(model.state_dict(), id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKlKTgSUi87i"
      },
      "source": [
        "# 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f5EsPfQ3Rio"
      },
      "source": [
        "print(\"Begin evaluation...\")\n",
        "start_time = time.time()\n",
        "test_loss = evaluate(model, test_batches, loss_func)\n",
        "print(f'Test loss: {test_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIvxfBSAz-K"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<E>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}