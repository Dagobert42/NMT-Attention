{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation with Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFyhR6JE0FSrG/Zoee/0+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagobert42/NMT-Attention/blob/main/Neural_Machine_Translation_with_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3O2RwGmK9M9"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The goal of this notebook is to implement the **RNNsearch-50** model, i.e., the encoder-decoder with attention system for any language pair different from English-German, German-English, English-French, and French-English. We choose **German-Italian** as an example language pair.\n",
        "\n",
        "Furthermore we loosely follow **test-driven development** (\"TDD\") paradigms to replicate the original system based on the paper:\n",
        "\n",
        "    Bahdanau, Cho & Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015.\n",
        "\n",
        "An in-depth walk-through of the project is given in the accompanying report which can be found here ADD LINK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhVOlKEwH9Ez"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "Please choose which dependencies to install to your environment. On re-runs you can ucheck the boxes to save some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz_I8H-r3VtB"
      },
      "source": [
        "torch = True #@param {type:\"boolean\"}\n",
        "if tf_datasets:\n",
        "    !pip install --upgrade torch\n",
        "\n",
        "torchtext = True #@param {type:\"boolean\"}\n",
        "if torchtext:\n",
        "    !pip install --upgrade torchtext\n",
        "\n",
        "spacy_packages = True #@param {type:\"boolean\"}\n",
        "if spacy_packs:\n",
        "    !pip install --upgrade spacy\n",
        "    !python -m spacy download en_core_news_sm\n",
        "    !python -m spacy download de_core_news_sm\n",
        "    !python -m spacy download it_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBfa-ne0NAB7"
      },
      "source": [
        "Next let us import all the modules we are going to be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nCXITKIM9wc"
      },
      "source": [
        "# access to translation datasets\n",
        "import torchtext\n",
        "\n",
        "# model implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# word tokenization\n",
        "import spacy\n",
        "\n",
        "# utilities\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOK7mD3p1uan"
      },
      "source": [
        "# 2. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBUzcOj_mRl"
      },
      "source": [
        "### 2.1 Dataset\n",
        "\n",
        "For data we refer to the Web Inventory of Transcribed and Translated Talks which comes as a torchtext dataset.  On re-runs you can ucheck the box to save some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI6oB9tZaDqe",
        "outputId": "f31e2ffc-d72e-4d66-e502-8426dfe7932b"
      },
      "source": [
        "from torchtext.datasets import IWSLT2017\n",
        "\n",
        "load_dataset = False #@param {type:\"boolean\"}\n",
        "if load_dataset:\n",
        "    train_iter, test_iter, val_iter = IWSLT2017(split=('train', 'test', 'valid'), language_pair=('de', 'it'))\n",
        "src_sentence, trg_sentence = next(train_iter)\n",
        "print('Example:\\n')\n",
        "print('source ->', src_sentence)\n",
        "print('target ->', trg_sentence)\n",
        "\n",
        "print('train examples:', len(train_iter), end='\\t')\n",
        "print('test:', len(test_iter), end='\\t')\n",
        "print('validation:', len(val_iter))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example:\n",
            "\n",
            "source -> Das meine ich ernst, teilweise deshalb -- weil ich es wirklich brauchen kann!\n",
            "\n",
            "target -> per i tanti, lusinghieri commenti, anche perch√©... Ne ho bisogno!!!\n",
            "\n",
            "train examples: 205465\ttest: 1567\tvalidation: 923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhSrQhBqtLra"
      },
      "source": [
        "### 2.2 Vocabulary\n",
        "\n",
        "The vocabulary converts words to indeces and vice versa. Unknown words are marked with the < U > token. The vocabulary can be filtered by word counts and provides methods for converting between sequences and sentences. It also takes an optional SpaCy nlp object which (when given) is used for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd2ZdAMQr_Er"
      },
      "source": [
        "class Vocab:\n",
        "    \"\"\"\n",
        "    A vocabulary holding dictionaries for converting words to indeces and back.\n",
        "    \"\"\"\n",
        "    class Entry:\n",
        "        def __init__(self, id):\n",
        "            \"\"\" An entry to the vocabulary. With index and count. \"\"\"\n",
        "            self.id = id\n",
        "            self.count = 1\n",
        "\n",
        "        def __repr__(self):\n",
        "            \"\"\" String prepresentation for printing. \"\"\"\n",
        "            return str((self.id, self.count))\n",
        "\n",
        "    def __init__(self, text=None, spacy_nlp=None):\n",
        "        \"\"\"\n",
        "        Creates a vocabulary over an input text in the form of\n",
        "        dictionaries for indeces and word counts. Hand in a\n",
        "        spacy_nlp object to make use of a SpaCy for tokenization.\n",
        "        \"\"\"\n",
        "        self.SPECIALS = '<S> <E> <U>'\n",
        "        self.spacy_nlp = spacy_nlp\n",
        "        self.words = {}\n",
        "        self.ids = {}\n",
        "\n",
        "        # add special tokens to vocabulary\n",
        "        for word in self.SPECIALS.split():\n",
        "            id = len(self.words.keys())\n",
        "            self.words[word] = self.Entry(id)\n",
        "            self.ids[id] = word\n",
        "\n",
        "        if text:\n",
        "            self.append(text)\n",
        "\n",
        "    def append(self, txt):\n",
        "        \"\"\" Adds a string token by token to the vocabulary. \"\"\"\n",
        "        # use SpaCy for tokenization if requested\n",
        "        if self.spacy_nlp:\n",
        "            for tok in self.spacy_nlp.tokenizer(txt):\n",
        "                word = tok.text\n",
        "                if tok.text not in self.words.keys():\n",
        "                    next_id = len(self.words.keys())\n",
        "                    self.words[word] = self.Entry(next_id)\n",
        "                    self.ids[next_id] = word\n",
        "                else:\n",
        "                    self.words[word].count += 1\n",
        "        else:\n",
        "            for word in txt.split():\n",
        "                if word not in self.words.keys():\n",
        "                    next_id = len(self.words.keys())\n",
        "                    self.words[word] = self.Entry(next_id)\n",
        "                    self.ids[next_id] = word\n",
        "                else:\n",
        "                    self.words[word].count += 1\n",
        "\n",
        "    def filter(self, n_samples, descending=True):\n",
        "        \"\"\"\n",
        "        Reduces this vocabs dictionary to n_samples\n",
        "        after sorting by word count. \n",
        "        \"\"\"\n",
        "        sorted_list = list(sorted(\n",
        "                self.words.items(),\n",
        "                key=lambda item: item[1].count,\n",
        "                reverse=descending))\n",
        "        self.words = {k: v for k, v in sorted_list[:n_samples]}\n",
        "        return self\n",
        "\n",
        "    def get_indeces(self, sentence):\n",
        "        \"\"\" Produces a representation from the indeces in this vocabulary. \"\"\"\n",
        "        STA = 0\n",
        "        END = 1\n",
        "        UNK = 2\n",
        "        if self.spacy_nlp:\n",
        "            seq = [self.words[word].id if word in self.words.keys() else UNK\n",
        "                for word in self.spacy_nlp.tokenizer(sentence)]\n",
        "        else:\n",
        "            seq = [self.words[word].id if word in self.words.keys() else UNK\n",
        "                for word in sentence.split()]\n",
        "        seq.append(END)\n",
        "        return seq\n",
        "    \n",
        "    def get_sentence(self, indeces):\n",
        "        \"\"\"\n",
        "        Converts a list of indeces into a readable sentence\n",
        "        using words from this vocabulary.\n",
        "        \"\"\"\n",
        "        return ' '.join([self.ids[id] for id in indeces])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmlGaPcWxy1B"
      },
      "source": [
        "We make sure the vocabulary works as intended by running tests with a tiny corpus. Note that during development these tests were written **first** and subsequently we implemented the related functionality in the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peRNwanhyNey"
      },
      "source": [
        "TEST_CORPUS_EN = \"\"\"The final project should implement a system \n",
        "        related to deep learning for NLP using the Py- Torch library \n",
        "        and test it. The project is documented in an ACL-style paper \n",
        "        that adheres to the standards of practice in computational \n",
        "        linguistics.\"\"\"\n",
        "\n",
        "test_vocab = Vocab(TEST_CORPUS_EN)\n",
        "\n",
        "# run some test on our implementation of vocabulary\n",
        "\n",
        "# BASIC\n",
        "assert(test_vocab.ids[0] == '<S>') # <S> should always be first\n",
        "assert(test_vocab.ids[3] == 'The')\n",
        "assert(test_vocab.words['The'].id == 3)\n",
        "assert(test_vocab.words['the'].count == 2)\n",
        "\n",
        "# SPACY OPTION\n",
        "english_nlp = spacy.load('en_core_web_sm')\n",
        "test_vocab = Vocab(TEST_CORPUS_EN, spacy_nlp=english_nlp)\n",
        "\n",
        "assert(test_vocab.ids[0] == '<S>') # <S> should always be first\n",
        "assert(test_vocab.ids[5] == 'project')\n",
        "assert(test_vocab.words['project'].id == 5)\n",
        "assert(test_vocab.words['project'].count == 2)\n",
        "\n",
        "# VOCAB FILTER\n",
        "top_30 = test_vocab.filter(n_samples=30)\n",
        "\n",
        "assert(len(test_vocab.words) == 30)\n",
        "assert(len(top_30.words) == 30)\n",
        "assert(top_30 == test_vocab)\n",
        "\n",
        "# SENT VEC CONVERSION\n",
        "vec = test_vocab.get_indeces(\"This is a test\")\n",
        "sent = test_vocab.get_sentence(vec)\n",
        "\n",
        "assert(vec == [2, 24, 8, 22, 1])\n",
        "assert(sent == '<U> is a test <E>')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB03OWaY1m-K"
      },
      "source": [
        "# 3. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOtwYOyD3D1J"
      },
      "source": [
        "## 3.1 Encoder\n"
      ]
    }
  ]
}